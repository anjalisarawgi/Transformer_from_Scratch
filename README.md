# Transformer_from_Scratch
Building the famous 2017 Transformer Encoder-Decoder from the Paper [Attention is All You Need](https://arxiv.org/abs/1706.03762)

## Features
Transformer Architecture: Complete implementation of the Transformer model, including multi-head self-attention and feed-forward neural network layers.\
Encoder-Decoder Structure: Implementation of both the encoder and decoder components as described in the paper.\
Attention Mechanism: Detailed implementation of the attention mechanism crucial to the Transformer's success.\

## Requirements
Python 3.x\
PyTorch

## Getting Started
Clone the repository: git clone https://github.com/yourusername/Transformer_from_Scratch.git    \
Install dependencies: install and import all the required packages in the first block of the code 

## References
Paper [Attention is All You Need]: Original paper detailing the Transformer model https://arxiv.org/abs/1706.03762 \
Youtube playlist on Transformers from scratch [Code Emporium]: https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4
